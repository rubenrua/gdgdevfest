# How To Use
---

This file should guide us throw all the steps needed in order to train, evaluate and use a custom object detection model.

We can take a look at `/jobs/sample_job` to explore the recommended structure. Ideally you should create a folder inside `jobs` for each dataset that you are going to train on. If you are a little lazy you can just modify the files in `/jobs/sample_job` to fit your needs.

All the code examples assume the following:

- We have created a copy of the `jobs/sample_job` folder and renamed that folder to our custom name (althoug in this file all code exmaples use `sample_job`, assuming that you felt lazy).

- We are running the code from the `object_detection_tensorflow` root folder.

- We have the `(tensorflow)` or `tensorflow-gpu` enviroment active (this enviroment should've been generated by the installation steps).

# 1 - Prepare Inputs

We need a dataset annotated with a bounding box and class label for each object that we want to detect in each image.

We can annotate images using our favourite labeling tool or we can download an annotated dataset from somewhere.

We recommend http://www.robots.ox.ac.uk/~vgg/software/via/via.html as labeling tool because it doesn't require any installation and we can annotate on the browser.

Once we have the dataset we need to generate a .record file for each split of the dataset (i.e train/eval) as follows:

Fill `/jobs/sample_job/data/label_map.pbtxt` with the information of our dataset. It's important to note that the indices *must* start at 1.

Assuming that:

- We have all our train images inside `jobs/sample_job/data/train` 
- The corresponding annotations generated by the vgg annotation tool are in `jobs/sample_job/data/train/annotations.json`
- The "region_attribute" where we stored the class information is named "Class"

We can run:

```bash
python object_detection/dataset_tools/create_vgg_tf_record.py \
--annotations jobs/sample_job/data/train/annotations.json \
--label_map jobs/sample_job/data/label_map.pbtxt \
--images jobs/sample_job/data/train \
--output jobs/sample_job/data/train.record
--class_field Class
```

This will generate the file `jobs/sample_job/data/train.record`.

If we have a eval split (we should) just run the same command replacing train by eval.

# 2 - Define the configuration file.

The Tensorflow Object Detection API uses protobuf files to configure the training and evaluation process.

Samples of configuration files for all the different pretrained architectures that the API offers are in `jobs/sample_job/configs`.

At a high level, the config file is split into 5 parts:

## model 

The provided default configurations are suitable for most use cases and we should only have to modify the `num_classes` and the `height` and `width` (inside `image_resizer`) parameters.

If we want to modify more parameters check the documentation for each of the avaliable meta-architectures:

    a) ssd

All the avaliable parameters are documented in `object_detection/protos/ssd.proto`.

    b) faster_rcnn

All the avaliable parameters are documented in `object_detection/protos/faster_rcnn.proto`.

**Note**

In order to understand some of the parameters you might need to be familiar with the terminology used in the papers associated with each model architecture.

Some parameters might have it's own parameters documented in other .proto files (i.e the `matcher` parameter is documented in `object_detection/protos/matcher.proto` and each of it's options are documented in `object_detection/protos/argmax_matcher.proto` and `object_detection/protos/bipartite_matcher.proto`)

## train_config

Decides what parameters should be used to train model parameters.

All the avaliable parameters are documented in `object_detection/protos/train.proto`.

**Note**

The `data_augmentation_options` parameter has a little counter-intuitive syntax, we have to repeat the parameter for each option:

```
  data_augmentation_options {
    random_horizontal_flip {
    }
  }
  data_augmentation_options {
    ssd_random_crop {
    }
  }
```

## train_input_reader

Defines what dataset the model should be trained on.

All the avaliable parameters are documented in `object_detection/protos/input_reader.proto`.

## eval_config

Determines what set of metrics will be reported for evaluation (currently we only support the PASCAL VOC metrics).

All the avaliable parameters are documented in `object_detection/protos/eval.proto`.

## eval_input_reader

Defines what dataset the model will be evaluated on. Typically this should be different than the training input dataset.

All the avaliable parameters are documented in `object_detection/protos/input_reader.proto`.

# 3 - Run the train and eval scripts

Once we have the .record and .config files we can launch the train and eval scripts in diferent terminals.

Assuming that we have modified the provided ssd_mobilenet.config sample to fit our needs:

```bash
python object_detection/train.py \
--logtostderr \
--pipeline_config_path jobs/sample_job/configs/ssd_mobilenet_v1.config \
--train_dir jobs/sample_job/train_out/ssd_mobilenet_v1
```
This script will start the training loop and save the checkpoints and logs to `--train_dir`

If there are not checkpoints saved in `--train_dir` (i.e the first time you run the script), the script will look for a COCO pretrained model under the pretrained folder if you haven't indicated the oposite in the configuration file. 

If there are checkpoints in `--train_dir` (i.e you interrupted the training and later you want to resume the process) the script will load the latest checkpoint and continue the training from it.

```bash
python object_detection/eval.py \
--logtostderr \
--pipeline_config_path jobs/sample_job/configs/ssd_mobilenet_v1.config \
--checkpoint_dir jobs/sample_job/train_out/ssd_mobilenet_v1 \
--eval_dir jobs/sample_job/eval_out/ssd_mobilenet_v1
```

This script will load the checkpoints saved in `--checkpoint_dir` (should equal to the previous `--train_dir`) and generate evaluation logs to `--eval_dir`.

You can use Tensorboard to visualize the train and eval results by running in **different terminals**:

```bash
tensorboard --logdir jobs/sample_job/train_out/
```

And

```bash
tensorboard --logdir jobs/sample_job/eval_out/ --port 6007
```
Note that we are forcing tensorboard to use the port 6007 in order to avoid collision with the tensorboard of the train process, wich uses the default (6006).

If you have run training and evaluation scripts for multiple architectures all of them should be visible in tensorboard with each one having different colors.

# 4 - Export inference graph

Once we are happy with our evaluation results or if we see that the training loss is converging, we can use the checkpoints of `--train_dir` in order to deploy the model anywhere. 

Assuming that we are going to use the checkpoint number 666 (you can select whatever checkpoint you want), we can generate a freezed graph optimized for inference as follows:

```bash
python object_detection/export_inference_graph.py \
--input_type image_tensor \
--pipeline_config_path jobs/sample_job/configs/ssd_mobilenet_v1.config \
--trained_checkpoint_prefix jobs/sample_job/train_out/ssd_mobilenet_v1/model.ckpt-666 \
--output_directory jobs/sample_job/train_out/ssd_mobilenet_v1/exported_graph
```

# 5 - Using the exported inference graph

Inside `--output_directory` there should be a file called `frozen_inference_graph.pb`.

This file contains the weights of our trained model along with the graph operations optimized for inference. We can use this file in several ways:

## a) Jupyter Notebooks 

We can run:

```bash
jupyter notebook
```

And navigate to the notebooks folder. There we can open `explore_model.ipynb` to we can load a model and explore how it generates predictions and process some test images.

## b)  Process Images

We can process a folder of images (`--input`) and save the corresponding images with the predicted boxes drawn in another folder (`--output`):


```bash
python object_detection/process_folder.py \
--model jobs/sample_job/train_out/ssd_mobilenet_v1/exported_graph/frozen_inference_graph.pb \
--input test_images \
--output test_images_processed \
--labels jobs/sample_job/data/label_map.pbtxt \
--n_classes 2
```

## c)  Process Video

We can process a video (`--input`) and generate a new video (`--output`) where each frame has the predicted boxes drawn:


```bash
python object_detection/process_video.py \
--model jobs/sample_job/train_out/ssd_mobilenet_v1/exported_graph/frozen_inference_graph.pb \
--input our_video.mp4 \
--output "our_video_processed.avi" \
--labels jobs/sample_job/data/label_map.pbtxt \
--n_classes 2
```

## d)  Process Webcam

We can process a video stream coming from a webcam and visualize the output:

```bash
python object_detection/process_webcam.py \
--model pretrained/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb \
--labels object_detection/data/mscoco_label_map.pbtxt \
--n_classes 90
```
